{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19abbdc-6249-4457-bcde-8bbc1f75490a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: mindspore 2.3.0\n",
      "Uninstalling mindspore-2.3.0:\n",
      "  Successfully uninstalled mindspore-2.3.0\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Collecting mindspore==2.3.0\n",
      "  Downloading https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.3.0/MindSpore/unified/aarch64/mindspore-2.3.0-cp39-cp39-linux_aarch64.whl (273.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (5.27.3)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (2.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (10.4.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (24.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (5.9.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from mindspore==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: six in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.3.0) (0.43.0)\n",
      "Installing collected packages: mindspore\n",
      "Successfully installed mindspore-2.3.0\n"
     ]
    }
   ],
   "source": [
    "# %%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.3.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "!export MINDSPORE_VERSION=2.3.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.mirrors.ustc.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c48cbf5-678f-40bb-98d0-6edb9103f2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Collecting jieba\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=aea641b9a1f6fb67e643986fe5190e8d3d5076bf6a48a3ee0ca21844e68e456e\n",
      "  Stored in directory: /home/mindspore/.cache/pip/wheels/06/74/c0/43b175e9228f8213c84801781a2afd74e1453f94677ca44004\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Collecting tiktoken\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/38/1a/f2a8928e6088e47073ddf6869ea733f54d21320f3ceec4aaf3440572a816/tiktoken-0.7.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/3a/72/2667206ef348d8e4cdd349d6f0e064002e8d393b66093181d1a865b94a11/regex-2024.7.24-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (777 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.7.24 tiktoken-0.7.0\n",
      "env: HF_ENDPOINT=https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "!pip install tiktoken -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "%env HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392ba82b-e8b7-4e2a-9acf-98d803279313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-29 07:13:10--  https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/model/ZhipuAI/glm-4-9b-chat.zip\n",
      "Resolving mindspore-demo.obs.cn-north-4.myhuaweicloud.com (mindspore-demo.obs.cn-north-4.myhuaweicloud.com)... 100.125.224.5\n",
      "Connecting to mindspore-demo.obs.cn-north-4.myhuaweicloud.com (mindspore-demo.obs.cn-north-4.myhuaweicloud.com)|100.125.224.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14885997618 (14G) [application/zip]\n",
      "Saving to: ‘glm-4-9b-chat.zip’\n",
      "\n",
      "glm-4-9b-chat.zip   100%[===================>]  13.86G  72.6MB/s    in 6m 52s  \n",
      "\n",
      "2024-08-29 07:20:02 (34.5 MB/s) - ‘glm-4-9b-chat.zip’ saved [14885997618/14885997618]\n",
      "\n",
      "Archive:  glm-4-9b-chat.zip\n",
      "   creating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/\n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00003-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00007-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model.safetensors.index.json  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/tokenizer.model  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/config.json  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00006-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00001-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00009-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00010-of-00010.safetensors  \n",
      " extracting: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/.zip  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00005-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00002-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00004-of-00010.safetensors  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/tokenizer_config.json  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/generation_config.json  \n",
      "  inflating: ./.mindnlp/model/ZhipuAI/glm-4-9b-chat/model-00008-of-00010.safetensors  \n"
     ]
    }
   ],
   "source": [
    "!wget https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/model/ZhipuAI/glm-4-9b-chat.zip\n",
    "!mkdir -p ./.mindnlp/model/ZhipuAI\n",
    "!unzip -d ./.mindnlp/model/ZhipuAI/ glm-4-9b-chat.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e02bc8a-54eb-45cc-99ec-18cb3892a174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.028 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.common.api import _no_grad\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mindnlp.core.optim import AdamW\n",
    "from mindnlp import evaluate\n",
    "from mindnlp.dataset import load_dataset\n",
    "from mindnlp.engine import set_seed\n",
    "from mindnlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from mindnlp.transformers.optimization import get_linear_schedule_with_warmup\n",
    "from mindnlp.peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    LoraConfig,\n",
    "    PromptEncoderConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824891e4-f714-45ec-9d64-895aabc37a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindnlp\n",
    "from mindnlp.core import no_grad\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from mindspore._c_expression import _framework_profiler_step_start\n",
    "from mindspore._c_expression import _framework_profiler_step_end\n",
    "# 加载tokenizer文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec95b20e-b10b-495c-b4d9-a885f9594d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_name_or_path = \"./.mindnlp/model/ZhipuAI/glm-4-9b-chat\"\n",
    "task = \"mrpc\"\n",
    "# peft_type = PeftType.PROMPT_TUNING\n",
    "peft_type = PeftType.P_TUNING\n",
    "num_epochs = 20\n",
    "# peft_config = LoraConfig(task_type=\"SEQ_2_SEQ_LM\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "peft_config = PromptEncoderConfig(task_type=\"SEQ_2_SEQ_LM\", num_virtual_tokens=20, encoder_hidden_size=128)\n",
    "lr = 3e-4\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcf1768-c325-466b-bc35-45c1ab00ee6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94c5079721f4f0b92ac0fb984b5800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,785,280 || all params: 9,402,736,640 || trainable%: 0.02962201438410169\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, mirror='modelscope')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    mirror='modelscope',\n",
    "    ms_dtype=mindspore.float16,\n",
    ").eval()\n",
    "\n",
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "model = get_peft_model(model, peft_config)\n",
    "# print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8117bc91-0dbf-4959-a7f5-ce463af81182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(153:281472832564096,MainProcess):2024-08-29-07:57:53.221.580 [mindspore/dataset/engine/datasets.py:1217] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "dataset = ds.CSVDataset(dataset_files=\"./dataset/output.csv\", column_names=['questions', 'answers'], column_defaults=[])\n",
    "train_dataset, validation_dataset = dataset.shuffle(64).split([0.9, 0.1])\n",
    "# next(train_dataset.create_dict_iterator())\n",
    "# def add_text_label(ask, answer, label):\n",
    "#     return ask, answer, classes[label.item()]\n",
    "\n",
    "# train_dataset = train_dataset.map(add_text_label, ['ask', 'answer', 'department'], ['ask', 'answer', 'text_label'])\n",
    "# validation_dataset = validation_dataset.map(add_text_label, ['ask', 'answer', 'department'], ['ask', 'answer', 'text_label'])\n",
    "\n",
    "# def add_text_label(ask, label):\n",
    "#     return ask, label, classes[label.item()]\n",
    "\n",
    "# train_dataset = train_dataset.map(add_text_label, ['ask', 'answer', 'department'], ['ask', 'answer', 'text_label'])\n",
    "# validation_dataset = validation_dataset.map(add_text_label, ['ask', 'answer', 'department'], ['ask', 'answer', 'text_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a4f74f-6df6-4aa4-885b-dd4222c8013b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindnlp.dataset import BaseMapFunction\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, inputs, outputs):\n",
    "        lock.acquire()\n",
    "        model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        labels = tokenizer(outputs, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        lock.release()\n",
    "        labels = labels['input_ids']\n",
    "        labels = np.where(np.equal(labels, tokenizer.pad_token_id), -100, labels)\n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels\n",
    "\n",
    "\n",
    "def get_dataset(dataset, tokenizer, shuffle=True):\n",
    "    input_colums=['questions', 'answers']\n",
    "    output_columns=['input_ids', 'attention_mask', 'labels']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(64)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset(train_dataset, tokenizer)\n",
    "eval_dataset = get_dataset(validation_dataset, tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb2ee63-acae-48fc-9e99-dda84b3eb34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "optimizer = mindnlp.core.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataset) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237f2d4-b131-4704-97e6-855e54147205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "def forward_fn(**batch):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, tuple(model.parameters()))\n",
    "\n",
    "def train_step(**batch):\n",
    "    loss, grads = grad_fn(**batch)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train()\n",
    "    total_loss = 0\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "        loss = train_step(**batch)\n",
    "        total_loss += loss.float()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        with mindspore._no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(ops.argmax(outputs.logits, -1).asnumpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataset)\n",
    "    eval_ppl = ops.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataset)\n",
    "    train_ppl = ops.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbf927-cc71-445a-83d0-7af56d96661f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
